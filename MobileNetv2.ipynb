{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T17:06:18.201194Z",
     "start_time": "2025-09-09T17:06:18.184086Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:06:18.919103Z",
     "start_time": "2025-09-09T17:06:18.898477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "allocated_bytes = torch.cuda.memory_allocated('cuda')\n",
    "allocated_mb = allocated_bytes / (1024 * 1024)\n",
    "\n",
    "reserved_bytes = torch.cuda.memory_reserved('cuda')\n",
    "reserved_mb = reserved_bytes / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- VRAM USAGE REPORT ---\")\n",
    "print(f\"Memory Allocated: {allocated_mb:.2f} MB\")\n",
    "print(f\"Memory Reserved:  {reserved_mb:.2f} MB\")\n",
    "print(\"-------------------------\\n\")"
   ],
   "id": "67c943da802f3366",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VRAM USAGE REPORT ---\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Reserved:  0.00 MB\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:18:54.983913Z",
     "start_time": "2025-09-09T17:18:54.967482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "validation_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "basic_pipeline = A.Compose([\n",
    "    # Resize the image first\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.RandomCrop(height=180, width=180, p=0.7),\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    A.VerticalFlip(p=0.6),\n",
    "    A.Rotate(limit=(-45,45), p=0.7),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "id": "6e6fca57e1d09951",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:11:48.815822Z",
     "start_time": "2025-09-09T17:11:48.811962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class PestClassifierResNet50(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super(PestClassifierResNet50,self).__init__()\n",
    "\n",
    "        self.base_model = models.resnet50(weights=None)\n",
    "\n",
    "        features = self.base_model.fc.in_features\n",
    "\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=False),\n",
    "            nn.Linear(features, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.base_model(x)"
   ],
   "id": "286f91b7e7faf653",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T14:44:49.811353Z",
     "start_time": "2025-09-09T14:44:47.964041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model = models.resnet50(weights=None)\n",
    "\n",
    "print(model)"
   ],
   "id": "d5daabe13160d1e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:19:10.907843Z",
     "start_time": "2025-09-09T17:19:10.897472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_and_validate(model, optimizer, criterion, train_dataloader, validation_dataloader, num_epoch, device):\n",
    "\n",
    "    total_time = 0.0\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    accuracies = []\n",
    "    best_validation_loss=float('inf')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Train epoch: {epoch+1}\")\n",
    "        for data, labels in train_pbar:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        training_loss = running_train_loss / len(train_dataloader)\n",
    "        train_losses.append(training_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_eval_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(validation_dataloader, desc=f\"Validate Epoch {epoch+1}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                running_eval_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        validation_loss = running_eval_loss / len(validation_dataloader)\n",
    "        validation_losses.append(validation_loss)\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        epoch_summary = f'Epoch {epoch+1}/{num_epoch} | Train Loss: {training_loss:.4f} | Evaluate Loss: {validation_loss:.4f} | Accuracy: {accuracy:.2f}%'\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            allocated_gb = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "            epoch_summary += f' | GPU Memory (Alloc/Reserved): {allocated_gb:.2f}/{reserved_gb:.2f} GB'\n",
    "\n",
    "        if validation_loss < best_validation_loss - 0.001:\n",
    "\n",
    "            best_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), 'MobileNetv2_result/models/model1.pt')\n",
    "\n",
    "        tqdm.write(epoch_summary)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "    return  {'train_loss': train_losses, 'validation_loss': validation_losses, 'accuracy': accuracies}, total_time"
   ],
   "id": "cc35e779d9c2a40b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:19:11.826122Z",
     "start_time": "2025-09-09T17:19:11.522036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from helpers import CustomDataset, plot_training_result, plot_accuracy, save_historical_data\n",
    "\n",
    "train_set = CustomDataset(root='datasets/train', transform=basic_pipeline)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "validation_set = CustomDataset(root='datasets/val', transform=validation_pipeline)\n",
    "validation_loader = DataLoader(validation_set, batch_size=4, shuffle=True)\n",
    "test_set = CustomDataset(root='datasets/test', transform=validation_pipeline)\n",
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=True)\n",
    "\n",
    "classes = train_set.classes"
   ],
   "id": "74fad95d0afbe791",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2> Hyper parameter tuning</h2>",
   "id": "5c2b0b5006c0ef0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:43:24.953967Z",
     "start_time": "2025-09-09T17:19:13.141726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "hardware = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
    "classes = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "model = PestClassifierResNet50(num_classes=classes).to(hardware)\n",
    "\n",
    "result, time = train_and_validate(model, optimizer, criterion, train_loader, validation_loader, num_epoch=16, device=hardware)\n",
    "\n",
    "print(f\"Training time: {time:.2f}\")\n",
    "\n",
    "plot_training_result(*result)\n",
    "plot_accuracy(*result)\n",
    "save_historical_data(*result, filename='MobileNetv2_result/hyper_parameter_tuning/model1.csv')"
   ],
   "id": "722bfc77df014ed2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 1: 100%|██████████| 592/592 [01:33<00:00,  6.30it/s]\n",
      "Validate Epoch 1: 100%|██████████| 676/676 [00:13<00:00, 49.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16 | Train Loss: 2.0373 | Evaluate Loss: 1.9941 | Accuracy: 12.54% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 2: 100%|██████████| 592/592 [01:13<00:00,  8.06it/s]\n",
      "Validate Epoch 2: 100%|██████████| 676/676 [00:10<00:00, 64.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/16 | Train Loss: 2.0439 | Evaluate Loss: 1.9926 | Accuracy: 11.21% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 3: 100%|██████████| 592/592 [01:13<00:00,  8.07it/s]\n",
      "Validate Epoch 3: 100%|██████████| 676/676 [00:10<00:00, 64.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/16 | Train Loss: 2.0389 | Evaluate Loss: 2.0073 | Accuracy: 12.68% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 4: 100%|██████████| 592/592 [01:11<00:00,  8.26it/s]\n",
      "Validate Epoch 4: 100%|██████████| 676/676 [00:10<00:00, 64.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/16 | Train Loss: 2.0398 | Evaluate Loss: 1.9909 | Accuracy: 12.02% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 5: 100%|██████████| 592/592 [01:11<00:00,  8.23it/s]\n",
      "Validate Epoch 5: 100%|██████████| 676/676 [00:10<00:00, 64.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/16 | Train Loss: 2.0418 | Evaluate Loss: 1.9795 | Accuracy: 11.65% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 6: 100%|██████████| 592/592 [01:11<00:00,  8.23it/s]\n",
      "Validate Epoch 6: 100%|██████████| 676/676 [00:10<00:00, 64.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/16 | Train Loss: 2.0358 | Evaluate Loss: 1.9994 | Accuracy: 11.95% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 7: 100%|██████████| 592/592 [01:12<00:00,  8.22it/s]\n",
      "Validate Epoch 7: 100%|██████████| 676/676 [00:10<00:00, 64.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/16 | Train Loss: 2.0380 | Evaluate Loss: 1.9934 | Accuracy: 12.09% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 8: 100%|██████████| 592/592 [01:12<00:00,  8.21it/s]\n",
      "Validate Epoch 8: 100%|██████████| 676/676 [00:10<00:00, 63.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/16 | Train Loss: 2.0471 | Evaluate Loss: 1.9917 | Accuracy: 11.43% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 9: 100%|██████████| 592/592 [01:15<00:00,  7.81it/s]\n",
      "Validate Epoch 9: 100%|██████████| 676/676 [00:13<00:00, 49.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/16 | Train Loss: 2.0399 | Evaluate Loss: 1.9961 | Accuracy: 12.24% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 10: 100%|██████████| 592/592 [01:22<00:00,  7.20it/s]\n",
      "Validate Epoch 10: 100%|██████████| 676/676 [00:11<00:00, 60.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/16 | Train Loss: 2.0392 | Evaluate Loss: 2.0125 | Accuracy: 12.02% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 11: 100%|██████████| 592/592 [01:35<00:00,  6.18it/s]\n",
      "Validate Epoch 11: 100%|██████████| 676/676 [00:23<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/16 | Train Loss: 2.0443 | Evaluate Loss: 2.0180 | Accuracy: 12.94% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 12: 100%|██████████| 592/592 [01:48<00:00,  5.44it/s]\n",
      "Validate Epoch 12: 100%|██████████| 676/676 [00:13<00:00, 51.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/16 | Train Loss: 2.0344 | Evaluate Loss: 1.9856 | Accuracy: 12.35% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 13: 100%|██████████| 592/592 [01:21<00:00,  7.28it/s]\n",
      "Validate Epoch 13: 100%|██████████| 676/676 [00:10<00:00, 64.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/16 | Train Loss: 2.0454 | Evaluate Loss: 2.0081 | Accuracy: 12.61% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 14: 100%|██████████| 592/592 [01:11<00:00,  8.24it/s]\n",
      "Validate Epoch 14: 100%|██████████| 676/676 [00:10<00:00, 64.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/16 | Train Loss: 2.0476 | Evaluate Loss: 1.9956 | Accuracy: 11.83% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 15: 100%|██████████| 592/592 [01:11<00:00,  8.26it/s]\n",
      "Validate Epoch 15: 100%|██████████| 676/676 [00:10<00:00, 64.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/16 | Train Loss: 2.0368 | Evaluate Loss: 2.0079 | Accuracy: 12.02% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: 16: 100%|██████████| 592/592 [01:11<00:00,  8.24it/s]\n",
      "Validate Epoch 16: 100%|██████████| 676/676 [00:10<00:00, 62.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16 | Train Loss: 2.0413 | Evaluate Loss: 1.9887 | Accuracy: 12.20% | GPU Memory (Alloc/Reserved): 0.30/1.87 GB\n",
      "Training time: 1451.28\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     10\u001B[39m result, time = train_and_validate(model, optimizer, criterion, train_loader, validation_loader, num_epoch=\u001B[32m16\u001B[39m, device=hardware)\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[43mplot_training_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m plot_accuracy(*result)\n\u001B[32m     16\u001B[39m save_historical_data(*result, filename=\u001B[33m'\u001B[39m\u001B[33mMobileNetv2_result/hyper_parameter_tuning/model1.csv\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\rice_pest\\helpers.py:12\u001B[39m, in \u001B[36mplot_training_result\u001B[39m\u001B[34m(*results)\u001B[39m\n\u001B[32m      9\u001B[39m full_validation_loss = []\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     train_loss = \u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m(\u001B[33m'\u001B[39m\u001B[33mtrain_loss\u001B[39m\u001B[33m'\u001B[39m, [])\n\u001B[32m     13\u001B[39m     full_training_loss.extend(train_loss)\n\u001B[32m     14\u001B[39m     full_validation_loss.extend(result.get(\u001B[33m'\u001B[39m\u001B[33mvalidation_loss\u001B[39m\u001B[33m'\u001B[39m, []))\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
