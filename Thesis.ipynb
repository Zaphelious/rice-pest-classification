{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Cuda verification </h1>",
   "id": "59300a61728d2dd1"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T13:08:15.282598Z",
     "start_time": "2025-08-01T13:08:15.237557Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 173,
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>Augmentation pipeline </h1>",
   "id": "f4338befb71eb1a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:30:55.887641Z",
     "start_time": "2025-08-01T11:30:55.474800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "training_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.SquareSymmetry(p=0.5),\n",
    "    A.RandomCrop(height=180, width=180,  p=1.0),\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.RandomRain(slant_range=(-15,15), drop_length=15, drop_width=1, rain_type=\"default\", blur_value=7 ,p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.2, 0.2), p=0.3),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.GaussNoise(std_range=(0.1, 0.2), per_channel=True  ,p=0.5),\n",
    "    A.ColorJitter(brightness=(0.8, 1.1), contrast=(0.8, 1.1), saturation=(0.8, 1.1), hue=(-0.5, 0.5)),\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.25),\n",
    "                        hole_width_range=(0.1, 0.25), fill=0, p=0.5),\n",
    "        A.GridDropout(ratio=0.5, random_offset=True,  p=0.5)\n",
    "    ]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "validation_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "id": "c5c5dfd54b31be52",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Data preparation</h2>",
   "id": "8c508484763306ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:47:01.813480Z",
     "start_time": "2025-08-01T13:47:01.803408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_image_datasets(source_dir, base_dir, split_ratios=(0.7, 0.2, 0.1)):\n",
    "\n",
    "     if not (0.999 < sum(split_ratios) < 1.001):\n",
    "         raise ValueError('split_ratios must sum to 1')\n",
    "\n",
    "     source_path = Path(source_dir)\n",
    "     base_path = Path(base_dir)\n",
    "\n",
    "     if not source_path.is_dir():\n",
    "         print(f'Source directory {source_path.name} does not exist')\n",
    "         return\n",
    "\n",
    "     train_path = base_path / 'train'\n",
    "     test_path = base_path / 'test'\n",
    "     val_path = base_path / 'val'\n",
    "\n",
    "     class_names = [d.name for d in source_path.iterdir() if d.is_dir()]\n",
    "\n",
    "     if not class_names:\n",
    "         print(f'Source directory {source_path.name} does not contain any class names')\n",
    "         return\n",
    "\n",
    "     for directory in [train_path, test_path, val_path]:\n",
    "         for class_name in class_names:\n",
    "             (directory / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "     for class_name in class_names:\n",
    "\n",
    "         class_source_path = source_path / class_name\n",
    "\n",
    "         files = [f for f in class_source_path.iterdir() if f.is_file()]\n",
    "\n",
    "         random.shuffle(files)\n",
    "\n",
    "         total_files = len(files)\n",
    "         train_end = int(total_files * split_ratios[0])\n",
    "         test_end = train_end + int(total_files * split_ratios[1])\n",
    "\n",
    "         split_data = {\n",
    "             'train': (files[:train_end], train_path),\n",
    "             'test': (files[train_end:test_end], test_path),\n",
    "             'val': (files[test_end:], val_path)\n",
    "         }\n",
    "\n",
    "         print(f\"Copying {class_name} to {base_path}\")\n",
    "\n",
    "         for split_name, (file_list, destination_path) in split_data.items():\n",
    "\n",
    "             dest_class_path = destination_path / class_name\n",
    "\n",
    "             for file_path in tqdm(file_list, desc=f'Copying {split_name} files'):\n",
    "\n",
    "                 shutil.copy2(file_path, dest_class_path / file_path.name)\n",
    "\n"
   ],
   "id": "3f61bebddba66303",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:54:31.848252Z",
     "start_time": "2025-08-01T11:54:31.770691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "data = CustomDataset(root='dataset', transform=training_pipeline)\n",
    "\n",
    "# image, path = data[0]\n",
    "#\n",
    "# print(path)\n",
    "#\n",
    "# image = image.permute(1,2,0).numpy()\n",
    "#\n",
    "# image = (image * np\n",
    "#          .array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "# image = np.clip(image, 0, 1)\n",
    "# image = (image * 255).astype(np.uint8)\n",
    "#\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ],
   "id": "4165f31c2a079958",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:46:06.517510Z",
     "start_time": "2025-08-01T13:45:25.320755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "source_data_dir = r'C:\\Users\\LANCE\\OneDrive\\Desktop\\rice_pest\\Finalized_datasets'\n",
    "base_data_dir = r'C:\\Users\\LANCE\\OneDrive\\Desktop\\rice_pest\\dataset'\n",
    "\n",
    "if not source_data_dir or not base_data_dir:\n",
    "    raise ValueError('Source and base data directory not found')\n",
    "\n",
    "prepare_image_datasets(source_data_dir, base_data_dir, split_ratios=(0.7, 0.2, 0.1))\n"
   ],
   "id": "407974787d3ba3a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1362/1362 [00:09<00:00, 151.11it/s]\n",
      "Copying test files: 100%|██████████| 389/389 [00:02<00:00, 151.45it/s]\n",
      "Copying val files: 100%|██████████| 195/195 [00:01<00:00, 157.64it/s]\n",
      "Copying train files: 100%|██████████| 763/763 [00:05<00:00, 149.81it/s]\n",
      "Copying test files: 100%|██████████| 218/218 [00:01<00:00, 164.58it/s]\n",
      "Copying val files: 100%|██████████| 109/109 [00:00<00:00, 156.36it/s]\n",
      "Copying train files: 100%|██████████| 1204/1204 [00:07<00:00, 153.75it/s]\n",
      "Copying test files: 100%|██████████| 344/344 [00:02<00:00, 164.47it/s]\n",
      "Copying val files: 100%|██████████| 172/172 [00:01<00:00, 159.31it/s]\n",
      "Copying train files: 100%|██████████| 671/671 [00:04<00:00, 161.03it/s]\n",
      "Copying test files: 100%|██████████| 191/191 [00:01<00:00, 149.55it/s]\n",
      "Copying val files: 100%|██████████| 97/97 [00:00<00:00, 151.51it/s]\n",
      "Copying train files: 100%|██████████| 410/410 [00:02<00:00, 157.82it/s]\n",
      "Copying test files: 100%|██████████| 117/117 [00:00<00:00, 154.45it/s]\n",
      "Copying val files: 100%|██████████| 59/59 [00:00<00:00, 171.70it/s]\n"
     ]
    }
   ],
   "execution_count": 174
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
