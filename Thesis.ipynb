{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Cuda verification </h1>",
   "id": "59300a61728d2dd1"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T13:08:15.282598Z",
     "start_time": "2025-08-01T13:08:15.237557Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 173,
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>Augmentation pipeline </h1>",
   "id": "f4338befb71eb1a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:30:55.887641Z",
     "start_time": "2025-08-01T11:30:55.474800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "training_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.SquareSymmetry(p=0.5),\n",
    "    A.RandomCrop(height=180, width=180,  p=1.0),\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.RandomRain(slant_range=(-15,15), drop_length=15, drop_width=1, rain_type=\"default\", blur_value=7 ,p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.2, 0.2), p=0.3),\n",
    "    A.Rotate(limit=45, p=0.5),\n",
    "    A.GaussNoise(std_range=(0.1, 0.2), per_channel=True  ,p=0.5),\n",
    "    A.ColorJitter(brightness=(0.8, 1.1), contrast=(0.8, 1.1), saturation=(0.8, 1.1), hue=(-0.5, 0.5)),\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.25),\n",
    "                        hole_width_range=(0.1, 0.25), fill=0, p=0.5),\n",
    "        A.GridDropout(ratio=0.5, random_offset=True,  p=0.5)\n",
    "    ]),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "validation_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "id": "c5c5dfd54b31be52",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Data preparation</h2>",
   "id": "8c508484763306ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:47:01.813480Z",
     "start_time": "2025-08-01T13:47:01.803408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_image_datasets(source_dir, base_dir, split_ratios=(0.7, 0.2, 0.1)):\n",
    "\n",
    "     if not (0.999 < sum(split_ratios) < 1.001):\n",
    "         raise ValueError('split_ratios must sum to 1')\n",
    "\n",
    "     source_path = Path(source_dir)\n",
    "     base_path = Path(base_dir)\n",
    "\n",
    "     if not source_path.is_dir():\n",
    "         print(f'Source directory {source_path.name} does not exist')\n",
    "         return\n",
    "\n",
    "     train_path = base_path / 'train'\n",
    "     test_path = base_path / 'test'\n",
    "     val_path = base_path / 'val'\n",
    "\n",
    "     class_names = [d.name for d in source_path.iterdir() if d.is_dir()]\n",
    "\n",
    "     if not class_names:\n",
    "         print(f'Source directory {source_path.name} does not contain any class names')\n",
    "         return\n",
    "\n",
    "     for directory in [train_path, test_path, val_path]:\n",
    "         for class_name in class_names:\n",
    "             (directory / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "     for class_name in class_names:\n",
    "\n",
    "         class_source_path = source_path / class_name\n",
    "\n",
    "         files = [f for f in class_source_path.iterdir() if f.is_file()]\n",
    "\n",
    "         random.shuffle(files)\n",
    "\n",
    "         total_files = len(files)\n",
    "         train_end = int(total_files * split_ratios[0])\n",
    "         test_end = train_end + int(total_files * split_ratios[1])\n",
    "\n",
    "         split_data = {\n",
    "             'train': (files[:train_end], train_path),\n",
    "             'test': (files[train_end:test_end], test_path),\n",
    "             'val': (files[test_end:], val_path)\n",
    "         }\n",
    "\n",
    "         print(f\"Copying {class_name} to {base_path}\")\n",
    "\n",
    "         for split_name, (file_list, destination_path) in split_data.items():\n",
    "\n",
    "             dest_class_path = destination_path / class_name\n",
    "\n",
    "             for file_path in tqdm(file_list, desc=f'Copying {split_name} files'):\n",
    "\n",
    "                 shutil.copy2(file_path, dest_class_path / file_path.name)\n"
   ],
   "id": "3f61bebddba66303",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:54:31.848252Z",
     "start_time": "2025-08-01T11:54:31.770691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "data = CustomDataset(root='dataset', transform=training_pipeline)\n",
    "\n",
    "# image, path = data[0]\n",
    "#\n",
    "# print(path)\n",
    "#\n",
    "# image = image.permute(1,2,0).numpy()\n",
    "#\n",
    "# image = (image * np\n",
    "#          .array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "# image = np.clip(image, 0, 1)\n",
    "# image = (image * 255).astype(np.uint8)\n",
    "#\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ],
   "id": "4165f31c2a079958",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2> Data splitting </h2>",
   "id": "638f3a60c11f293c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:46:06.517510Z",
     "start_time": "2025-08-01T13:45:25.320755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "source_data_dir = r'C:\\Users\\LANCE\\OneDrive\\Desktop\\rice_pest\\Finalized_datasets'\n",
    "base_data_dir = r'C:\\Users\\LANCE\\OneDrive\\Desktop\\rice_pest\\dataset'\n",
    "\n",
    "if not source_data_dir or not base_data_dir:\n",
    "    raise ValueError('Source and base data directory not found')\n",
    "\n",
    "prepare_image_datasets(source_data_dir, base_data_dir, split_ratios=(0.7, 0.2, 0.1))\n"
   ],
   "id": "407974787d3ba3a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying train files: 100%|██████████| 1362/1362 [00:09<00:00, 151.11it/s]\n",
      "Copying test files: 100%|██████████| 389/389 [00:02<00:00, 151.45it/s]\n",
      "Copying val files: 100%|██████████| 195/195 [00:01<00:00, 157.64it/s]\n",
      "Copying train files: 100%|██████████| 763/763 [00:05<00:00, 149.81it/s]\n",
      "Copying test files: 100%|██████████| 218/218 [00:01<00:00, 164.58it/s]\n",
      "Copying val files: 100%|██████████| 109/109 [00:00<00:00, 156.36it/s]\n",
      "Copying train files: 100%|██████████| 1204/1204 [00:07<00:00, 153.75it/s]\n",
      "Copying test files: 100%|██████████| 344/344 [00:02<00:00, 164.47it/s]\n",
      "Copying val files: 100%|██████████| 172/172 [00:01<00:00, 159.31it/s]\n",
      "Copying train files: 100%|██████████| 671/671 [00:04<00:00, 161.03it/s]\n",
      "Copying test files: 100%|██████████| 191/191 [00:01<00:00, 149.55it/s]\n",
      "Copying val files: 100%|██████████| 97/97 [00:00<00:00, 151.51it/s]\n",
      "Copying train files: 100%|██████████| 410/410 [00:02<00:00, 157.82it/s]\n",
      "Copying test files: 100%|██████████| 117/117 [00:00<00:00, 154.45it/s]\n",
      "Copying val files: 100%|██████████| 59/59 [00:00<00:00, 171.70it/s]\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2> Training functions and hyperparameters</h2>",
   "id": "dba860140f2d1e9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T14:07:52.628164Z",
     "start_time": "2025-08-03T14:07:52.620863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LSoftmax(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features, number_of_classes, m=4):\n",
    "        super(LSoftmax, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.number_of_classes = number_of_classes\n",
    "        self.m = m\n",
    "\n",
    "        if m <= 1:\n",
    "            raise ValueError('m must be greater than 1')\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(input_features, number_of_classes))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "\n",
    "        normalize_weight = F.normalize(self.weight, p=2, dim=1)\n",
    "        normalize_x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        cos_theta = F.linear(normalize_x, normalize_weight.t())\n",
    "        cos_theta = cos_theta.clamp(-1,1)\n",
    "\n",
    "        theta = torch.acos(cos_theta)\n",
    "\n",
    "        target_theta = theta.gather(1, labels.view(-1,1))\n",
    "\n",
    "        m_theta = self.m * target_theta\n",
    "        k = (m_theta / math.pi).floor()\n",
    "\n",
    "        psi_theta = ((-1)**k) * torch.cos(m_theta) - (2*k)\n",
    "\n",
    "        final_logits = cos_theta.scatter(1, labels.view(-1,1), psi_theta)\n",
    "\n",
    "        return final_logits\n"
   ],
   "id": "31a9a78b6df933f4",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T08:35:51.302500Z",
     "start_time": "2025-08-03T08:35:51.292451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))"
   ],
   "id": "eec42032231e0ff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> LSmax Test </h3>",
   "id": "8abca27bc20b3b89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T14:13:28.962845Z",
     "start_time": "2025-08-03T14:13:28.957393Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output logits: torch.Size([4, 6])\n",
      "\n",
      "Output Logits Tensor (first 2 rows):\n",
      "[[-2.2175212  -0.46530092  0.4388179  -0.13423419  0.6960334   0.20117837]\n",
      " [ 0.06377348  0.31151363  0.52479124  0.12865415 -3.1037827   0.04310565]]\n"
     ]
    }
   ],
   "execution_count": 277,
   "source": [
    "NUM_FEATURES = 1280\n",
    "NUM_CLASSES = 6\n",
    "BATCH_SIZE = 4\n",
    "MARGIN = 4\n",
    "\n",
    "lsoftmax_layer = LSoftmax(\n",
    "    input_features=NUM_FEATURES,\n",
    "    number_of_classes=NUM_CLASSES,\n",
    "    m=MARGIN\n",
    ")\n",
    "\n",
    "dummy_features = torch.randn(BATCH_SIZE, NUM_FEATURES)\n",
    "dummy_labels = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,))\n",
    "res = lsoftmax_layer(dummy_features, dummy_labels)\n",
    "\n",
    "print(\"Shape of the output logits:\", res.shape)\n",
    "print(\"\\nOutput Logits Tensor (first 2 rows):\")\n",
    "print(res.detach().numpy()[:2])"
   ],
   "id": "42f054b187673f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec3d396b8b938549"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
