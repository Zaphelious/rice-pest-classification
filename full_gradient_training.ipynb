{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Experimentation of Full Gradient training </h1>",
   "id": "963de395a05581f3"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T16:01:25.067076Z",
     "start_time": "2025-08-10T16:01:24.976064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from sched import scheduler\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check and report VRAM usage\n",
    "# torch.cuda.memory_allocated() gives the current tensor memory usage in bytes.\n",
    "allocated_bytes = torch.cuda.memory_allocated('cuda')\n",
    "allocated_mb = allocated_bytes / (1024 * 1024)\n",
    "\n",
    "reserved_bytes = torch.cuda.memory_reserved('cuda')\n",
    "reserved_mb = reserved_bytes / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- VRAM USAGE REPORT ---\")\n",
    "print(f\"Memory Allocated: {allocated_mb:.2f} MB\")\n",
    "print(f\"Memory Reserved:  {reserved_mb:.2f} MB\")\n",
    "print(\"-------------------------\\n\")\n",
    "#"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VRAM USAGE REPORT ---\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Reserved:  0.00 MB\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:01:28.836376Z",
     "start_time": "2025-08-10T16:01:28.724315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "training_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.SquareSymmetry(p=0.5),\n",
    "    A.RandomCrop(height=180, width=180,  p=0.8),\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.RandomRain(slant_range=(-15,15), drop_length=15, drop_width=1, rain_type=\"default\", blur_value=7 ,p=0.4),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.2, 0.2), p=0.3),\n",
    "    A.Rotate(limit=45, p=0.4),\n",
    "    A.GaussNoise(std_range=(0.1, 0.2), per_channel=True  ,p=0.3),\n",
    "    A.ColorJitter(brightness=(0.8, 1.1), contrast=(0.8, 1.1), saturation=(0.8, 1.1), hue=(-0.5, 0.5)),\n",
    "    A.CoarseDropout(num_holes_range=(1, 5), hole_height_range=(0.1, 0.25),\n",
    "                        hole_width_range=(0.1, 0.25), fill=0, p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "validation_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "experiment_pipeline = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    A.VerticalFlip(p=0.6),\n",
    "    A.Rotate(limit=(-45,45), p=0.7),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "id": "1b63162625f1ad43",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:01:32.788244Z",
     "start_time": "2025-08-10T16:01:32.779418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomDataset(datasets.ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ],
   "id": "5761b938ad2d55d2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:02:11.214552Z",
     "start_time": "2025-08-10T16:02:11.210154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class PestClassifierMobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(PestClassifierMobileNetV2, self).__init__()\n",
    "\n",
    "        self.base_model = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V2')\n",
    "\n",
    "        # for params in self.base_model.parameters():\n",
    "        #     params.requires_grad = False\n",
    "\n",
    "        num_filters = self.base_model.classifier[1].in_features\n",
    "\n",
    "        self.base_model.classifier[1] = nn.Linear(num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "       features = self.base_model.features(x)\n",
    "\n",
    "       features = F.adaptive_avg_pool2d(features, output_size=(1, 1)).reshape(features.shape[0], -1)\n",
    "\n",
    "       features = self.base_model.classifier(features)\n",
    "\n",
    "       return features\n",
    "\n",
    "    def freeze_layers(self):\n",
    "\n",
    "        print(\"Parameter frozen\")\n",
    "\n",
    "        for params in self.base_model.parameters():\n",
    "            params.requires_grad = False"
   ],
   "id": "f7cb6f97185f4973",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:02:59.478891Z",
     "start_time": "2025-08-10T16:02:59.471673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_and_test(model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, num_epoch, device):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    accuracies = []\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Train epoch: {epoch+1}\")\n",
    "        for data, labels in train_pbar:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        training_loss = running_train_loss / len(train_dataloader)\n",
    "        train_losses.append(training_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_dataloader, desc=f\"Test epoch: {epoch+1}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                output_loss = model(inputs)\n",
    "                loss = criterion(output_loss, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                _, predicted = torch.max(output_loss.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        testing_loss = running_test_loss / len(test_dataloader)\n",
    "        test_losses.append(testing_loss)\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        epoch_summary = f'Epoch {epoch+1}/{num_epoch} | Train Loss: {training_loss:.4f} | Test Loss: {testing_loss:.4f} | Accuracy: {accuracy:.2f}%'\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            allocated_gb = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "            epoch_summary += f' | GPU Memory (Alloc/Reserved): {allocated_gb:.2f}/{reserved_gb:.3f} GB'\n",
    "\n",
    "        tqdm.write(epoch_summary)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'models/FMobileNetV2_rice_pest_classificationV1.pt')\n",
    "\n",
    "        scheduler.step(testing_loss)\n",
    "\n",
    "    # tqdm.write('Phase done!')\n",
    "\n",
    "    return {'train_loss': train_losses, 'test_loss': test_losses, 'accuracy': accuracies}, best_accuracy"
   ],
   "id": "923461bef47be1bc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:37:07.595269Z",
     "start_time": "2025-08-10T16:37:07.567269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_result(*results):\n",
    "\n",
    "    full_train_loss = []\n",
    "    full_test_loss = []\n",
    "\n",
    "    for res in results:\n",
    "        full_train_loss.extend(res.get('train_loss', []))\n",
    "        full_test_loss.extend(res.get('test_loss', []))\n",
    "\n",
    "    total_epochs = len(full_train_loss)\n",
    "    if total_epochs == 0:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(1, total_epochs + 1)\n",
    "\n",
    "    data = {\n",
    "        'Epoch': epochs,\n",
    "        'Training Loss': full_train_loss,\n",
    "        'Testing Loss': full_test_loss,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df_long = df.melt(id_vars=['Epoch'], var_name='Loss Type', value_name='Loss')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_long, x='Epoch', y='Loss', hue='Loss Type', marker='o')\n",
    "\n",
    "    plt.title('Training and Testing Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(*results):\n",
    "\n",
    "    full_accuracy = []\n",
    "\n",
    "    for result in results:\n",
    "        accuracy = result.get('accuracy', [])\n",
    "        full_accuracy.extend(accuracy)\n",
    "\n",
    "    total_epochs = len(full_accuracy)\n",
    "    if total_epochs == 0:\n",
    "        print(\"No accuracy data to plot.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(1, total_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, full_accuracy, label='Accuracy', marker='o', color='g')\n",
    "\n",
    "    plt.title('Accuracy Evaluation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_historical_data(*histories, filename='full_gradient_training_history_standardV1.csv'):\n",
    "\n",
    "    full_train_loss = []\n",
    "    full_test_loss = []\n",
    "    full_accuracy = []\n",
    "\n",
    "    for history in histories:\n",
    "\n",
    "        full_train_loss.extend(history.get('train_loss', []))\n",
    "        full_test_loss.extend(history.get('test_loss', []))\n",
    "        full_accuracy.extend(history.get('accuracy', []))\n",
    "\n",
    "    data_dict = pd.DataFrame({\n",
    "        'train_loss': full_train_loss,\n",
    "        'test_loss': full_test_loss,\n",
    "        'accuracy': full_accuracy,\n",
    "    })\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Existing historical data exists! Appending new data.\")\n",
    "\n",
    "        old_data = pd.read_csv(filename)\n",
    "        combined_data = pd.concat([old_data, data_dict], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(\"Creating new historical data...\")\n",
    "        combined_data = data_dict\n",
    "\n",
    "    combined_data['epoch'] = range(1, len(combined_data) + 1)\n",
    "\n",
    "    combined_data = combined_data[['epoch', 'train_loss', 'test_loss', 'accuracy']]\n",
    "\n",
    "    combined_data.to_csv(filename, index=False)\n",
    "\n",
    "    print(\"Historical data saved successfully!\")\n"
   ],
   "id": "c6096fc3ed630bb0",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T16:16:22.014627Z",
     "start_time": "2025-08-10T16:16:22.004714Z"
    }
   },
   "cell_type": "code",
   "source": "print(range(1, 6, + 1))",
   "id": "f0f7551220b5d81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 6)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T15:47:02.833049Z",
     "start_time": "2025-08-10T15:47:02.302754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = CustomDataset(root='datasets/train', transform=training_pipeline)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_set = CustomDataset(root='datasets/test', transform=validation_pipeline)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "classes = train_set.classes"
   ],
   "id": "7b2db18d9d88202",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1> Full gradient training: implementation of lr scheduler </h1>",
   "id": "239a462553235fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2> 64 epoch training </h2>",
   "id": "cd4f52fbef964e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "hardware = torch.device('cuda' if   torch.cuda.is_available() else 'cpu')\n",
    "classes = 6\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "num_epochs = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = PestClassifierMobileNetV2(num_classes=classes).to(hardware)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=4)\n",
    "\n",
    "result, accuracies = train_and_test(model=model, optimizer=optimizer, criterion=criterion,train_dataloader=train_loader, test_dataloader=test_loader, num_epoch=num_epochs, device=hardware)\n",
    "\n",
    "plot_training_result(result)\n",
    "plot_accuracy(result)\n",
    "save_historical_data(result)"
   ],
   "id": "a81236827dd33bf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_single_image(model, image_path, classes, transform, device):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    augmented = transform(image=image_np)\n",
    "\n",
    "    image_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        result = model(image_tensor)\n",
    "\n",
    "        probability = F.softmax(result, dim=1)\n",
    "\n",
    "        confidence, predicted = torch.max(probability, 1)\n",
    "\n",
    "        predicted_class = classes[predicted.item()]\n",
    "        confidence_score = confidence.item()\n",
    "\n",
    "    return predicted_class, confidence_score\n"
   ],
   "id": "cde7bbf589723d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "classes_names = train_set.classes\n",
    "hardware = torch.device('cuda' if   torch.cuda.is_available() else 'cpu')\n",
    "model_inference = PestClassifierMobileNetV2(num_classes=len(classes_names))\n",
    "model_inference.load_state_dict(torch.load('models/SMobileNetV2_rice_pest_classificationV8.pt', map_location=hardware))\n",
    "image_path = 'unrelated_data/grasshoper.jpg'\n",
    "\n",
    "pest, confidence_score = predict_single_image(model=model_inference, image_path=image_path, classes=classes_names, transform=validation_pipeline, device=hardware)\n",
    "\n",
    "print(f\"The predicted pest is: {pest}\")\n",
    "print(f\"Confidence: {confidence_score * 100:.2f}%\")"
   ],
   "id": "bafc66a6afdd5c72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
